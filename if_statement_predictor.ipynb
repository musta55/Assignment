{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import parso\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "# Updated imports for the new tokenizer\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from transformers import (\n",
    "    T5Config,\n",
    "    T5ForConditionalGeneration,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# 1. CONFIGURATION\n",
    "\n",
    "GITHUB_API_KEY = os.getenv(\"GITHUB_API_KEY\")\n",
    "PRETRAIN_TARGET_SIZE = 150000   \n",
    "FINETUNE_TARGET_SIZE = 50000    \n",
    "NUM_REPOS_TO_FETCH = 20       \n",
    "MIN_FUNCTION_TOKENS = 20      \n",
    "TOKENIZER_VOCAB_SIZE = 20000\n",
    "MAX_TRAIN_EPOCHS_PRETRAIN = 3\n",
    "MAX_TRAIN_EPOCHS_FINETUNE = 5\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACCUM = 32\n",
    "MAX_SEQ_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 128\n",
    "EVAL_STEPS = 2000\n",
    "SAVE_STEPS = 2000\n",
    "\n",
    "MASK_TOKEN = \"<extra_id_0>\"\n",
    "\n",
    "DATA_ROOT = Path(\"./if_statement_project\")\n",
    "RAW_CODE_DIR = DATA_ROOT / \"raw_code\"\n",
    "TOKENIZER_PATH = DATA_ROOT / \"python_tokenizer.json\"\n",
    "ALL_FUNCTIONS_FILE = DATA_ROOT / \"all_functions.txt\"\n",
    "PRETRAIN_FILE = DATA_ROOT / \"pretrain.txt\"\n",
    "PRETRAIN_VALID_FILE = DATA_ROOT / \"pretrain_valid.txt\"\n",
    "FINETUNE_TRAIN_FILE = DATA_ROOT / \"finetune_train.txt\"\n",
    "FINETUNE_VALID_FILE = DATA_ROOT / \"finetune_valid.txt\"\n",
    "FINETUNE_TEST_FILE = DATA_ROOT / \"finetune_test.txt\"\n",
    "PRETRAINED_MODEL_DIR = DATA_ROOT / \"pretrained_t5\"\n",
    "FINETUNED_MODEL_DIR = DATA_ROOT / \"finetuned_if_model\"\n",
    "PROVIDED_TESTSET_PATH = DATA_ROOT / \"provided_testset_to_process.csv\"\n",
    "\n",
    "DATA_ROOT.mkdir(exist_ok=True)\n",
    "RAW_CODE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# 2. DATA COLLECTION & PROCESSING\n",
    "\n",
    "def get_popular_python_repos(n=NUM_REPOS_TO_FETCH):\n",
    "    \"\"\"Fetches the most starred Python repositories from GitHub.\"\"\"\n",
    "    print(f\"Fetching {n} popular Python repository names...\")\n",
    "    headers = {\"Authorization\": f\"Bearer {GITHUB_API_KEY}\"}\n",
    "    params = {\"q\": \"language:Python\", \"sort\": \"stars\", \"order\": \"desc\", \"per_page\": n}\n",
    "    resp = requests.get(\"https://api.github.com/search/repositories\", headers=headers, params=params)\n",
    "    resp.raise_for_status()\n",
    "    return [repo['full_name'] for repo in resp.json()['items']]\n",
    "\n",
    "def get_python_files_from_repo(repo_full_name):\n",
    "    \"\"\"Recursively fetches all .py file paths from a repository.\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {GITHUB_API_KEY}\"}\n",
    "    api_url = f\"https://api.github.com/repos/{repo_full_name}/git/trees/main?recursive=1\"\n",
    "    resp = requests.get(api_url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        return []\n",
    "    tree = resp.json().get('tree', [])\n",
    "    py_files = [item['path'] for item in tree if item['path'].endswith('.py')]\n",
    "    return py_files\n",
    "\n",
    "def download_and_save_code(repo_full_name, file_path):\n",
    "    \"\"\"Downloads a single file's content and saves it locally.\"\"\"\n",
    "    raw_url = f\"https://raw.githubusercontent.com/{repo_full_name}/main/{file_path}\"\n",
    "    try:\n",
    "        resp = requests.get(raw_url, timeout=10)\n",
    "        if resp.status_code == 200:\n",
    "            safe_filename = file_path.replace('/', '_')\n",
    "            with open(RAW_CODE_DIR / f\"{repo_full_name.replace('/', '_')}_{safe_filename}\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(resp.text)\n",
    "            return True\n",
    "    except (requests.exceptions.RequestException, Exception):\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def extract_functions_from_code(code_str):\n",
    "    \"\"\"Uses parso to extract function bodies from a string of Python code.\"\"\"\n",
    "    try:\n",
    "        module = parso.parse(code_str)\n",
    "        return [\n",
    "            node.get_code()\n",
    "            for node in module.iter_funcdefs()\n",
    "            if len(node.get_code().split()) > MIN_FUNCTION_TOKENS\n",
    "        ]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# --- 3. TOKENIZER TRAINING ---\n",
    "\n",
    "def train_tokenizer(file_path):\n",
    "    \"\"\"Trains a Hugging Face BPE tokenizer from the collected functions.\"\"\"\n",
    "    print(\"Training Hugging Face BPE tokenizer...\")\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=TOKENIZER_VOCAB_SIZE,\n",
    "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", MASK_TOKEN]\n",
    "    )\n",
    "\n",
    "    tokenizer.train(files=[str(file_path)], trainer=trainer)\n",
    "    tokenizer.save(str(TOKENIZER_PATH))\n",
    "    print(\"Tokenizer training complete.\")\n",
    "\n",
    "# --- 4. DATASET CREATION ---\n",
    "\n",
    "def create_pretrain_dataset(functions, tokenizer):\n",
    "    \"\"\"Creates MLM dataset for T5 span corruption.\"\"\"\n",
    "    print(f\"Creating pre-training dataset with {PRETRAIN_TARGET_SIZE} instances...\")\n",
    "    dataset = []\n",
    "    pbar = tqdm(total=PRETRAIN_TARGET_SIZE)\n",
    "    \n",
    "    attempts = 0\n",
    "    max_attempts = PRETRAIN_TARGET_SIZE * 10  # Prevent infinite loop\n",
    "    \n",
    "    while len(dataset) < PRETRAIN_TARGET_SIZE and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        func = random.choice(functions)\n",
    "        \n",
    "        tokens = tokenizer.encode(func).tokens\n",
    "        if len(tokens) < 10:\n",
    "            continue\n",
    "\n",
    "        num_to_mask = max(1, int(len(tokens) * 0.15))\n",
    "        \n",
    "        masked_indices = sorted(random.sample(range(len(tokens)), k=min(num_to_mask, len(tokens))))\n",
    "        \n",
    "        input_parts = []\n",
    "        target_parts = []\n",
    "        \n",
    "        last_end = 0\n",
    "        mask_counter = 0\n",
    "        for i in masked_indices:\n",
    "            input_parts.extend(tokens[last_end:i])\n",
    "            input_parts.append(f\"<extra_id_{mask_counter}>\")\n",
    "            target_parts.append(f\"<extra_id_{mask_counter}>\")\n",
    "            target_parts.append(tokens[i])\n",
    "            mask_counter += 1\n",
    "            last_end = i + 1\n",
    "        \n",
    "        input_parts.extend(tokens[last_end:])\n",
    "        target_parts.append(f\"<extra_id_{mask_counter}>\")\n",
    "\n",
    "        input_str = \" \".join(input_parts)\n",
    "        target_str = \" \".join(target_parts)\n",
    "        \n",
    "        dataset.append(f\"{input_str}\\t{target_str}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    \n",
    "    if len(dataset) < PRETRAIN_TARGET_SIZE:\n",
    "        print(f\"Warning: Only created {len(dataset)} examples instead of {PRETRAIN_TARGET_SIZE}\")\n",
    "    \n",
    "    # Split dataset into train (90%) and validation (10%) for pre-training\n",
    "    split_idx = int(len(dataset) * 0.9)\n",
    "    train_data = dataset[:split_idx]\n",
    "    valid_data = dataset[split_idx:]\n",
    "    \n",
    "    # Save train and validation separately\n",
    "    with open(PRETRAIN_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(train_data))\n",
    "    \n",
    "    pretrain_valid_file = DATA_ROOT / \"pretrain_valid.txt\"\n",
    "    with open(pretrain_valid_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(valid_data))\n",
    "    \n",
    "    print(f\"Pre-training: {len(train_data)} train, {len(valid_data)} validation samples\")\n",
    "\n",
    "def create_finetune_dataset(functions):\n",
    "    \"\"\"Creates the if-statement masking dataset.\"\"\"\n",
    "    print(f\"Creating fine-tuning dataset with {FINETUNE_TARGET_SIZE} instances...\")\n",
    "    dataset = []\n",
    "    pbar = tqdm(total=FINETUNE_TARGET_SIZE)\n",
    "    \n",
    "    if_regex = re.compile(r'if\\s+(.+?):')\n",
    "    \n",
    "    random.shuffle(functions)\n",
    "    for func in functions:\n",
    "        matches = list(if_regex.finditer(func))\n",
    "        if not matches:\n",
    "            continue\n",
    "            \n",
    "        match = random.choice(matches)\n",
    "        condition = match.group(1).strip()\n",
    "        \n",
    "        masked_func = func[:match.start(1)] + MASK_TOKEN + func[match.end(1):]\n",
    "        \n",
    "        dataset.append(f\"{masked_func}\\t{condition}\")\n",
    "        pbar.update(1)\n",
    "        if len(dataset) >= FINETUNE_TARGET_SIZE:\n",
    "            break\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    if len(dataset) < FINETUNE_TARGET_SIZE:\n",
    "        print(f\"Warning: Only created {len(dataset)} examples instead of {FINETUNE_TARGET_SIZE}\")\n",
    "    \n",
    "    # Split and save\n",
    "    train_end = int(len(dataset) * 0.8)\n",
    "    valid_end = int(len(dataset) * 0.9)\n",
    "    \n",
    "    with open(FINETUNE_TRAIN_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(dataset[:train_end]))\n",
    "    with open(FINETUNE_VALID_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(dataset[train_end:valid_end]))\n",
    "    with open(FINETUNE_TEST_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(dataset[valid_end:]))\n",
    "\n",
    "# --- 5. MODEL TRAINING ---\n",
    "\n",
    "def run_training(is_pretraining):\n",
    "    \"\"\"A general function to run either pre-training or fine-tuning.\"\"\"\n",
    "    \n",
    "    model_dir = PRETRAINED_MODEL_DIR if is_pretraining else FINETUNED_MODEL_DIR\n",
    "    train_file = PRETRAIN_FILE if is_pretraining else FINETUNE_TRAIN_FILE\n",
    "    valid_file = PRETRAIN_FILE if is_pretraining else FINETUNE_VALID_FILE\n",
    "    \n",
    "    print(f\"\\n--- Starting {'Pre-training' if is_pretraining else 'Fine-tuning'} ---\")\n",
    "\n",
    "    # Clear CUDA cache if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Load custom tokenizer\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=str(TOKENIZER_PATH))\n",
    "    tokenizer.pad_token = \"<pad>\"\n",
    "    \n",
    "    # Load or initialize model\n",
    "    if is_pretraining:\n",
    "        config = T5Config(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            d_model= 256,  \n",
    "            d_ff= 1024,\n",
    "            num_layers= 4,   \n",
    "            num_heads= 4,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        model = T5ForConditionalGeneration(config)\n",
    "        print(f\"Initialized model with {model.num_parameters():,} parameters\")\n",
    "    else:\n",
    "        model = T5ForConditionalGeneration.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "\n",
    "    # Load and process dataset\n",
    "    dataset = load_dataset('text', data_files={'train': str(train_file), 'validation': str(valid_file)})\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        source, target = [], []\n",
    "        for line in examples['text']:\n",
    "            if '\\t' in line:\n",
    "                s, t = line.split('\\t', 1)\n",
    "                source.append(s)\n",
    "                target.append(t)\n",
    "        \n",
    "        if not source:  # Handle empty batches\n",
    "            return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "        \n",
    "        prefix = \"complete the python code: \"\n",
    "        source = [prefix + s for s in source]\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            source, \n",
    "            max_length=MAX_SEQ_LENGTH, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True\n",
    "        )\n",
    "        labels = tokenizer(\n",
    "            target, \n",
    "            max_length=MAX_TARGET_LENGTH, \n",
    "            padding=\"max_length\", \n",
    "            truncation=True\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_function, \n",
    "        batched=True, \n",
    "        remove_columns=['text'],\n",
    "        desc=\"Tokenizing\"\n",
    "    )\n",
    "\n",
    "    # Training arguments\n",
    "    num_epochs = MAX_TRAIN_EPOCHS_PRETRAIN if is_pretraining else MAX_TRAIN_EPOCHS_FINETUNE\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(model_dir),\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        num_train_epochs=num_epochs,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=EVAL_STEPS,\n",
    "        save_steps=SAVE_STEPS,\n",
    "        learning_rate=5e-4 if is_pretraining else 1e-4,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,\n",
    "        predict_with_generate=True,\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps= 500,\n",
    "        fp16=True,  # Use mixed precision for faster training\n",
    "        dataloader_num_workers=0,\n",
    "        report_to=\"none\",  # Disable wandb/tensorboard\n",
    "        max_grad_norm=1.0,  # Gradient clipping\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Train with error handling\n",
    "    try:\n",
    "        print(f\"Starting training for {num_epochs} epoch(s)...\")\n",
    "        print(f\"Training samples: {len(tokenized_datasets['train'])}\")\n",
    "        print(f\"Validation samples: {len(tokenized_datasets['validation'])}\")\n",
    "        trainer.train()\n",
    "        trainer.save_model()\n",
    "        print(f\"--- {'Pre-training' if is_pretraining else 'Fine-tuning'} Complete ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# --- 7. MAIN EXECUTION (Steps 1-5) ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Pretrain samples: {PRETRAIN_TARGET_SIZE}\")\n",
    "    print(f\"  Finetune samples: {FINETUNE_TARGET_SIZE}\")\n",
    "    print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"  Gradient accumulation: {GRAD_ACCUM}\")\n",
    "    print(f\"  Pretrain epochs: {MAX_TRAIN_EPOCHS_PRETRAIN}\")\n",
    "    print(f\"  Finetune epochs: {MAX_TRAIN_EPOCHS_FINETUNE}\\n\")\n",
    "    \n",
    "    # --- Step 1: Collect and Process Data ---\n",
    "    if not list(RAW_CODE_DIR.glob(\"*.py\")):\n",
    "        repo_names = get_popular_python_repos()\n",
    "        for repo_name in tqdm(repo_names, desc=\"Downloading Repos\"):\n",
    "            py_files = get_python_files_from_repo(repo_name)\n",
    "            for file_path in tqdm(py_files, desc=f\"Files in {repo_name}\", leave=False):\n",
    "                download_and_save_code(repo_name, file_path)\n",
    "    \n",
    "    if not ALL_FUNCTIONS_FILE.exists():\n",
    "        all_funcs = []\n",
    "        for code_file in tqdm(list(RAW_CODE_DIR.glob(\"*.py\")), desc=\"Extracting Functions\"):\n",
    "            with open(code_file, 'r', encoding='utf-8') as f:\n",
    "                all_funcs.extend(extract_functions_from_code(f.read()))\n",
    "        \n",
    "        all_funcs = list(set(all_funcs))  # Deduplicate\n",
    "        with open(ALL_FUNCTIONS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(all_funcs))\n",
    "        print(f\"Extracted and saved {len(all_funcs)} unique functions.\")\n",
    "    else:\n",
    "        with open(ALL_FUNCTIONS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_funcs = f.read().splitlines()\n",
    "        print(f\"Loaded {len(all_funcs)} functions from cache.\")\n",
    "\n",
    "    # --- Step 2: Train Tokenizer ---\n",
    "    if not TOKENIZER_PATH.exists():\n",
    "        train_tokenizer(ALL_FUNCTIONS_FILE)\n",
    "\n",
    "    # --- Step 3: Create Datasets ---\n",
    "    if not PRETRAIN_FILE.exists() or not FINETUNE_TRAIN_FILE.exists():\n",
    "        tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "        create_pretrain_dataset(all_funcs, tokenizer)\n",
    "        create_finetune_dataset(all_funcs)\n",
    "    \n",
    "    # --- Step 4: Pre-train the Model ---\n",
    "    if not (PRETRAINED_MODEL_DIR / \"pytorch_model.bin\").exists():\n",
    "        run_training(is_pretraining=True)\n",
    "\n",
    "    # --- Step 5: Fine-tune the Model ---\n",
    "    if not (FINETUNED_MODEL_DIR / \"pytorch_model.bin\").exists():\n",
    "        run_training(is_pretraining=False)\n",
    "    \n",
    "    print(\"\\n--- Steps 1-5 Complete. Run next cell for evaluation. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. EVALUATION ---\n",
    "\n",
    "def load_provided_test_set(file_path):\n",
    "    \"\"\"Loads and parses the provided test CSV with specific format.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['code'].tolist()\n",
    "\n",
    "\n",
    "def evaluate_and_create_csv(model, tokenizer, test_data, output_filename):\n",
    "    \"\"\"Runs model predictions and saves results to a CSV file.\"\"\"\n",
    "    print(f\"Evaluating and creating {output_filename}...\")\n",
    "\n",
    "    results = []\n",
    "    device = \"cuda\"\n",
    "    model.to(device)\n",
    "    if_regex = re.compile(r'if\\s+(.+?):')\n",
    "\n",
    "    for code in tqdm(test_data, desc=f\"Predicting for {output_filename}\"):\n",
    "        matches = list(if_regex.finditer(code))\n",
    "        if not matches:\n",
    "            continue\n",
    "\n",
    "        match = matches[0]\n",
    "        expected_condition = match.group(1).strip()\n",
    "\n",
    "        input_code = code[:match.start(1)] + MASK_TOKEN + code[match.end(1):]\n",
    "        prefix = \"complete the python code: \"\n",
    "\n",
    "        # FIX: Stop returning token_type_ids which T5 doesn't support\n",
    "        inputs = tokenizer(\n",
    "            prefix + input_code,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            truncation=True,\n",
    "        ).to(device)\n",
    "\n",
    "        # Only forward valid fields\n",
    "        input_kwargs = {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"]\n",
    "        }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **input_kwargs,\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                num_beams=5,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "\n",
    "        predicted_condition = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "        seq_log_prob = output.sequences_scores[0].item()\n",
    "        score = min(100.0, max(0.0, torch.exp(torch.tensor(seq_log_prob)).item() * 100))\n",
    "\n",
    "        results.append({\n",
    "            \"Input provided to the model\": input_code,\n",
    "            \"Whether the prediction is correct\": predicted_condition.strip() == expected_condition,\n",
    "            \"Expected if condition\": expected_condition,\n",
    "            \"Predicted if condition\": predicted_condition,\n",
    "            \"Prediction score (0-100)\": round(score, 2)\n",
    "        })\n",
    "\n",
    "    if not results:\n",
    "        print(f\"WARNING: No valid test cases found for {output_filename}\")\n",
    "        return\n",
    "\n",
    "    pd.DataFrame(results).to_csv(output_filename, index=False)\n",
    "    print(f\"Successfully saved {len(results)} results to {output_filename}\")\n",
    "\n",
    "    correct = sum(1 for r in results if r[\"Whether the prediction is correct\"])\n",
    "    accuracy = (correct / len(results)) * 100\n",
    "    print(f\"Accuracy: {accuracy:.2f}% ({correct}/{len(results)})\")\n",
    "\n",
    "\n",
    "# --- 7. MAIN EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"\\n--- Starting Final Evaluation ---\")\n",
    "    final_model = T5ForConditionalGeneration.from_pretrained(FINETUNED_MODEL_DIR)\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=str(TOKENIZER_PATH))\n",
    "    tokenizer.pad_token = \"<pad>\"\n",
    "\n",
    "    # Evaluate generated fine-tune test set\n",
    "    with open(FINETUNE_TEST_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        generated_test_data = [line.split('\\t')[0] for line in f.read().splitlines()]\n",
    "    evaluate_and_create_csv(final_model, tokenizer, generated_test_data, \"generated-testset.csv\")\n",
    "\n",
    "    # Evaluate user-provided test set\n",
    "    if PROVIDED_TESTSET_PATH.exists():\n",
    "        provided_test_data = load_provided_test_set(PROVIDED_TESTSET_PATH)\n",
    "        evaluate_and_create_csv(final_model, tokenizer, provided_test_data, \"provided-testset.csv\")\n",
    "    else:\n",
    "        print(f\"Warning: Provided test set missing at '{PROVIDED_TESTSET_PATH}' — skipping evaluation.\")\n",
    "\n",
    "    print(\"\\n--- PIPELINE COMPLETE ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
