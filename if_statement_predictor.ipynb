{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import tqdm\n",
    "import parso\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "GITHUB_API_KEY = os.getenv(\"GITHUB_API_KEY\")\n",
    "GITHUB_SEARCH_URL = \"https://api.github.com/search/repositories\"\n",
    "NUMBER_OF_REPOS = 50\n",
    "MIN_FUNCTION_LENGTH = 20\n",
    "DATA_ROOT = Path(\"./python_if_dataset\")\n",
    "all_functions_path = DATA_ROOT / \"all_functions.txt\"\n",
    "\n",
    "with open(all_functions_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    funcs = f.readlines()\n",
    "\n",
    "PRETRAIN_SIZE = 150000\n",
    "FINETUNE_SIZE = 50000\n",
    "MASK_TOKEN = \"<extra_id_0>\"\n",
    "TOKENIZER_VOCAB_SIZE = 32000\n",
    "REPOS_PER_PAGE = 30\n",
    "\n",
    "def show_dir(path, label):\n",
    "    print(f\"{label}: {path}\")\n",
    "    for name in os.listdir(path):\n",
    "        print(\" -\", name)\n",
    "\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "(DATA_ROOT / \"raw\").mkdir(parents=True, exist_ok=True)\n",
    "show_dir(DATA_ROOT, \"After directory creation\")\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GITHUB_API_KEY}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "def github_search_python_repos(n=30):\n",
    "    print(\"Searching Python repos on GitHub...\")\n",
    "    repos = []\n",
    "    per_page = min(n, REPOS_PER_PAGE)\n",
    "    pages = (n + per_page - 1) // per_page\n",
    "    for page in range(1, pages+1):\n",
    "        to_fetch = min(per_page, n - len(repos))\n",
    "        params = {\"q\": \"language:Python\", \"sort\": \"stars\", \"order\": \"desc\",\n",
    "                  \"per_page\": to_fetch, \"page\": page}\n",
    "        resp = requests.get(GITHUB_SEARCH_URL, headers=HEADERS, params=params, timeout=20)\n",
    "        print(\"API page\", page, \"status\", resp.status_code)\n",
    "        if resp.status_code != 200:\n",
    "            print(resp.text)\n",
    "            raise RuntimeError(f\"GitHub API error {resp.status_code}\")\n",
    "        data = resp.json()\n",
    "        repos.extend(data.get(\"items\", []))\n",
    "        if len(repos) >= n:\n",
    "            break\n",
    "    print(f\"Returning {len(repos)} repo URLs\")\n",
    "    return [r[\"clone_url\"] for r in repos[:n]]\n",
    "\n",
    "def clone_repo(url):\n",
    "    local_dir = DATA_ROOT / \"raw\" / url.split('/')[-1].replace('.git','')\n",
    "    if not local_dir.exists():\n",
    "        print(f\"Cloning: {url}\")\n",
    "        os.system(f'git clone --depth=1 {url} \"{local_dir}\" > /dev/null 2>&1')\n",
    "    return local_dir\n",
    "\n",
    "def extract_functions_from_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "        module = parso.parse(source)\n",
    "        return [node.get_code() for node in module.iter_funcdefs() if node.type == 'funcdef']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {filepath}, {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_functions_from_repo(repo_dir):\n",
    "    py_files = list(repo_dir.rglob(\"*.py\"))\n",
    "    print(f\"{repo_dir}: found {len(py_files)} .py files\")\n",
    "    funcs = []\n",
    "    for file in tqdm.tqdm(py_files, desc=f\"Parsing {repo_dir.name}\"):\n",
    "        funs = extract_functions_from_file(file)\n",
    "        funcs.extend(f for f in funs if len(f) > MIN_FUNCTION_LENGTH)\n",
    "    print(f\"{repo_dir}: extracted {len(funcs)} functions\")\n",
    "    return funcs\n",
    "\n",
    "print(\"Fetching repos...\")\n",
    "repo_urls = github_search_python_repos(NUMBER_OF_REPOS)\n",
    "print(\"Repo URLs:\", repo_urls[:3], \"...\")\n",
    "print(f\"Found {len(repo_urls)} repos.\")\n",
    "\n",
    "funcs = []\n",
    "for url in tqdm.tqdm(repo_urls, desc=\"Downloading & extracting functions\"):\n",
    "    repo_path = clone_repo(url)\n",
    "    funcs.extend(extract_functions_from_repo(repo_path))\n",
    "\n",
    "print(f\"Extracted {len(funcs)} functions.\")\n",
    "show_dir(DATA_ROOT, \"After extraction\")\n",
    "assert len(funcs) > 0, \"No functions found!\"\n",
    "\n",
    "all_functions_path = DATA_ROOT / \"all_functions.txt\"\n",
    "with open(all_functions_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for func in funcs:\n",
    "        f.write(func.strip() + \"\\n\")\n",
    "print(\"Wrote all_functions.txt:\", os.path.exists(all_functions_path), \"size:\",\n",
    "      os.path.getsize(all_functions_path))\n",
    "\n",
    "# SentencePiece-related code removed because sentencepiece is not used\n",
    "\n",
    "# The rest of your pipeline like creating masked pairs and saving them would need to be rewritten, \n",
    "# possibly using a different tokenizer or text processing approach if needed.\n",
    "\n",
    "print(\"Completed pipeline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello, World2!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Step 1: Train tokenizer on all_functions.txt (run once)\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(files=[str(all_functions_path)], vocab_size=32000, special_tokens=[\"<mask>\", \"<pad>\", \"<unk>\"])\n",
    "tokenizer_save_dir = DATA_ROOT / \"tokenizer\"\n",
    "os.makedirs(tokenizer_save_dir, exist_ok=True)  # Create directory if missing\n",
    "\n",
    "tokenizer.save_model(str(tokenizer_save_dir))\n",
    "\n",
    "# Step 2-3: Load tokenizer and prepare masked language model data\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    vocab=str(DATA_ROOT / \"tokenizer\" / \"vocab.json\"),\n",
    "    merges=str(DATA_ROOT / \"tokenizer\" / \"merges.txt\"),\n",
    ")\n",
    "\n",
    "MASK_TOKEN = \"<mask>\"\n",
    "mask_token_id = tokenizer.token_to_id(MASK_TOKEN)\n",
    "\n",
    "mlm_pairs = []\n",
    "for func in funcs:\n",
    "    tokens = tokenizer.encode(func).ids\n",
    "    num_to_mask = max(1, int(0.15 * len(tokens)))\n",
    "    mask_indices = random.sample(range(len(tokens)), num_to_mask)\n",
    "    masked_tokens = [mask_token_id if i in mask_indices else t for i, t in enumerate(tokens)]\n",
    "    input_text = tokenizer.decode(masked_tokens)\n",
    "    output_text = func\n",
    "    mlm_pairs.append((input_text, output_text))\n",
    "    if len(mlm_pairs) >= 150000:\n",
    "        break\n",
    "\n",
    "# Step 4: Create fine-tuning data with one masked if-condition\n",
    "finetune_pairs = []\n",
    "import re\n",
    "\n",
    "def mask_if_condition(func_code):\n",
    "    conditions = re.findall(r'(if\\s+)([^\\:]+)(\\:)', func_code)\n",
    "    if not conditions:\n",
    "        return None\n",
    "    cond = random.choice(conditions)\n",
    "    masked_func = func_code.replace(f\"{cond[0]}{cond[1]}{cond[2]}\", f\"{cond[0]}{MASK_TOKEN}{cond[2]}\", 1)\n",
    "    return masked_func, cond[1].strip()\n",
    "\n",
    "for func in funcs:\n",
    "    pair = mask_if_condition(func)\n",
    "    if pair:\n",
    "        finetune_pairs.append(pair)\n",
    "    if len(finetune_pairs) >= 50000:\n",
    "        break\n",
    "\n",
    "# Shuffle and split finetune_pairs into train/valid/test\n",
    "random.shuffle(finetune_pairs)\n",
    "n = len(finetune_pairs)\n",
    "train, valid, test = finetune_pairs[:int(0.8*n)], finetune_pairs[int(0.8*n):int(0.9*n)], finetune_pairs[int(0.9*n):]\n",
    "\n",
    "# Step 5: Save datasets\n",
    "def save_dataset(pairs, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for inp, outp in pairs:\n",
    "            f.write(f\"{inp}\\t{outp}\\n\")\n",
    "\n",
    "save_dataset(mlm_pairs, DATA_ROOT / \"pretrain_mlm.txt\")\n",
    "save_dataset(train, DATA_ROOT / \"finetune_train.txt\")\n",
    "save_dataset(valid, DATA_ROOT / \"finetune_valid.txt\")\n",
    "save_dataset(test, DATA_ROOT / \"finetune_test.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39fa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Pre-training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_724994/276032571.py:287: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'pad_token_id': 1}.\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 398\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# --- Step 4: Pre-train the Model ---\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (PRETRAINED_MODEL_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m--> 398\u001b[0m      \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_pretraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# --- Step 5: Fine-tune the Model ---\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (FINETUNED_MODEL_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists():\n",
      "Cell \u001b[0;32mIn[13], line 297\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(is_pretraining)\u001b[0m\n\u001b[1;32m    287\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m    288\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    289\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m    294\u001b[0m )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPre-training\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mis_pretraining\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFine-tuning\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/transformers/trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4026\u001b[0m ):\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/transformers/trainer.py:4110\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   4109\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m-> 4110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   4112\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   4113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:184\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule must have its parameters and buffers \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon device \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (device_ids[0]) but found one of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m         )\n\u001b[0;32m--> 184\u001b[0m inputs, module_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inputs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_kwargs:\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py:208\u001b[0m, in \u001b[0;36mDataParallel.scatter\u001b[0;34m(self, inputs, kwargs, device_ids)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscatter\u001b[39m(\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    204\u001b[0m     inputs: \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m],\n\u001b[1;32m    205\u001b[0m     kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m    206\u001b[0m     device_ids: Sequence[Union[\u001b[38;5;28mint\u001b[39m, torch\u001b[38;5;241m.\u001b[39mdevice]],\n\u001b[1;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:88\u001b[0m, in \u001b[0;36mscatter_kwargs\u001b[0;34m(inputs, kwargs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Scatter with support for kwargs dictionary.\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m scattered_inputs \u001b[38;5;241m=\u001b[39m scatter(inputs, target_gpus, dim) \u001b[38;5;28;01mif\u001b[39;00m inputs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m---> 88\u001b[0m scattered_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scattered_inputs) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(scattered_kwargs):\n\u001b[1;32m     90\u001b[0m     scattered_inputs\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m     91\u001b[0m         () \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(scattered_kwargs) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(scattered_inputs))\n\u001b[1;32m     92\u001b[0m     )\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:74\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(inputs, target_gpus, dim)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# After scatter_map is called, a scatter_map cell will exist. This cell\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# has a reference to the actual function scatter_map, which has references\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# to a closure that has a reference to the scatter_map cell (because the\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# fn is recursive). To avoid this reference cycle, we set the function to\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# None, clearing the cell\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mscatter_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     scatter_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:65\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscatter_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [obj \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m target_gpus]\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:61\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mscatter_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mlist\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/nn/parallel/scatter_gather.py:57\u001b[0m, in \u001b[0;36mscatter.<locals>.scatter_map\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mscatter_map\u001b[39m(obj):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mScatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_namedtuple(obj):\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mtype\u001b[39m(obj)(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(scatter_map, obj))]\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/autograd/function.py:581\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    587\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    589\u001b[0m     )\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:102\u001b[0m, in \u001b[0;36mScatter.forward\u001b[0;34m(ctx, target_gpus, chunk_sizes, dim, input)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39minput_device \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Perform CPU to GPU copies in a background stream\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     streams \u001b[38;5;241m=\u001b[39m [_get_stream(torch\u001b[38;5;241m.\u001b[39mdevice(device)) \u001b[38;5;28;01mfor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m target_gpus]\n\u001b[0;32m--> 102\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcomm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# Synchronize with the copy stream\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m streams \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Assignment/.venv/lib/python3.10/site-packages/torch/nn/parallel/comm.py:204\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(tensor, devices, chunk_sizes, dim, streams, out)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     devices \u001b[38;5;241m=\u001b[39m [_get_device_index(d) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m devices]\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m devices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import parso\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import load_dataset\n",
    "# Updated imports for the new tokenizer\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "from transformers import (\n",
    "    T5Config,\n",
    "    T5ForConditionalGeneration,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# IMPORTANT: Replace with your actual GitHub Personal Access Token\n",
    "GITHUB_API_KEY = os.getenv(\"GITHUB_API_KEY\")\n",
    "# --- Dataset and Model Parameters ---\n",
    "# NOTE: Sizes are reduced from the prompt for a faster run.\n",
    "# Change these back to 150000 and 50000 for the full assignment.\n",
    "PRETRAIN_TARGET_SIZE = 150000   \n",
    "FINETUNE_TARGET_SIZE = 50000   \n",
    "NUM_REPOS_TO_FETCH = 25        \n",
    "MIN_FUNCTION_TOKENS = 20      \n",
    "TOKENIZER_VOCAB_SIZE = 20000   \n",
    "MASK_TOKEN = \"<extra_id_0>\"   \n",
    "\n",
    "# --- File and Directory Paths ---\n",
    "DATA_ROOT = Path(\"./if_statement_project\")\n",
    "RAW_CODE_DIR = DATA_ROOT / \"raw_code\"\n",
    "# Corrected TOKENIZER_PATH to point to a .json file\n",
    "TOKENIZER_PATH = DATA_ROOT / \"python_tokenizer.json\"\n",
    "ALL_FUNCTIONS_FILE = DATA_ROOT / \"all_functions.txt\"\n",
    "PRETRAIN_FILE = DATA_ROOT / \"pretrain.txt\"\n",
    "FINETUNE_TRAIN_FILE = DATA_ROOT / \"finetune_train.txt\"\n",
    "FINETUNE_VALID_FILE = DATA_ROOT / \"finetune_valid.txt\"\n",
    "FINETUNE_TEST_FILE = DATA_ROOT / \"finetune_test.txt\"\n",
    "PRETRAINED_MODEL_DIR = DATA_ROOT / \"pretrained_t5\"\n",
    "FINETUNED_MODEL_DIR = DATA_ROOT / \"finetuned_if_model\"\n",
    "PROVIDED_TESTSET_PATH = DATA_ROOT / \"provided_testset_to_process.csv\"\n",
    "\n",
    "# --- Create Directories ---\n",
    "DATA_ROOT.mkdir(exist_ok=True)\n",
    "RAW_CODE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# --- 2. DATA COLLECTION & PROCESSING ---\n",
    "\n",
    "def get_popular_python_repos(n=NUM_REPOS_TO_FETCH):\n",
    "    \"\"\"Fetches the most starred Python repositories from GitHub.\"\"\"\n",
    "    print(f\"Fetching {n} popular Python repository names...\")\n",
    "    headers = {\"Authorization\": f\"Bearer {GITHUB_API_KEY}\"}\n",
    "    params = {\"q\": \"language:Python\", \"sort\": \"stars\", \"order\": \"desc\", \"per_page\": n}\n",
    "    resp = requests.get(\"https://api.github.com/search/repositories\", headers=headers, params=params)\n",
    "    resp.raise_for_status()\n",
    "    return [repo['full_name'] for repo in resp.json()['items']]\n",
    "\n",
    "def get_python_files_from_repo(repo_full_name):\n",
    "    \"\"\"Recursively fetches all .py file paths from a repository.\"\"\"\n",
    "    headers = {\"Authorization\": f\"Bearer {GITHUB_API_KEY}\"}\n",
    "    api_url = f\"https://api.github.com/repos/{repo_full_name}/git/trees/main?recursive=1\"\n",
    "    resp = requests.get(api_url, headers=headers)\n",
    "    if resp.status_code != 200:\n",
    "        return []\n",
    "    tree = resp.json().get('tree', [])\n",
    "    return [item['path'] for item in tree if item['path'].endswith('.py')]\n",
    "\n",
    "def download_and_save_code(repo_full_name, file_path):\n",
    "    \"\"\"Downloads a single file's content and saves it locally.\"\"\"\n",
    "    raw_url = f\"https://raw.githubusercontent.com/{repo_full_name}/main/{file_path}\"\n",
    "    try:\n",
    "        resp = requests.get(raw_url)\n",
    "        if resp.status_code == 200:\n",
    "            # Sanitize filename to avoid issues with slashes in paths\n",
    "            safe_filename = file_path.replace('/', '_')\n",
    "            with open(RAW_CODE_DIR / f\"{repo_full_name.replace('/', '_')}_{safe_filename}\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(resp.text)\n",
    "            return True\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def extract_functions_from_code(code_str):\n",
    "    \"\"\"Uses parso to extract function bodies from a string of Python code.\"\"\"\n",
    "    try:\n",
    "        module = parso.parse(code_str)\n",
    "        return [\n",
    "            node.get_code()\n",
    "            for node in module.iter_funcdefs()\n",
    "            if len(node.get_code().split()) > MIN_FUNCTION_TOKENS\n",
    "        ]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# --- 3. TOKENIZER TRAINING ---\n",
    "\n",
    "def train_tokenizer(file_path):\n",
    "    \"\"\"Trains a Hugging Face BPE tokenizer from the collected functions.\"\"\"\n",
    "    print(\"Training Hugging Face BPE tokenizer...\")\n",
    "    # Initialize a BPE tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "    # Configure the trainer\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=TOKENIZER_VOCAB_SIZE,\n",
    "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", MASK_TOKEN]\n",
    "    )\n",
    "\n",
    "    # Train the tokenizer\n",
    "    tokenizer.train(files=[str(file_path)], trainer=trainer)\n",
    "\n",
    "    # Save the tokenizer to the specified JSON file\n",
    "    tokenizer.save(str(TOKENIZER_PATH))\n",
    "    print(\"Tokenizer training complete.\")\n",
    "\n",
    "\n",
    "# --- 4. DATASET CREATION ---\n",
    "\n",
    "def create_pretrain_dataset(functions, tokenizer):\n",
    "    \"\"\"Creates MLM dataset for T5 span corruption.\"\"\"\n",
    "    print(f\"Creating pre-training dataset with {PRETRAIN_TARGET_SIZE} instances...\")\n",
    "    dataset = []\n",
    "    pbar = tqdm(total=PRETRAIN_TARGET_SIZE)\n",
    "    while len(dataset) < PRETRAIN_TARGET_SIZE:\n",
    "        func = random.choice(functions)\n",
    "        \n",
    "        # Correctly tokenize the function using the trained tokenizer\n",
    "        tokens = tokenizer.encode(func).tokens\n",
    "        if len(tokens) < 10: continue\n",
    "\n",
    "        num_to_mask = int(len(tokens) * 0.15)\n",
    "        if num_to_mask == 0: continue\n",
    "\n",
    "        masked_indices = sorted(random.sample(range(len(tokens)), k=num_to_mask))\n",
    "        \n",
    "        input_parts = []\n",
    "        target_parts = []\n",
    "        \n",
    "        last_end = 0\n",
    "        mask_counter = 0\n",
    "        for i in masked_indices:\n",
    "            input_parts.extend(tokens[last_end:i])\n",
    "            input_parts.append(f\"<extra_id_{mask_counter}>\")\n",
    "            target_parts.append(f\"<extra_id_{mask_counter}>\")\n",
    "            target_parts.append(tokens[i])\n",
    "            mask_counter += 1\n",
    "            last_end = i + 1\n",
    "        \n",
    "        input_parts.extend(tokens[last_end:])\n",
    "        target_parts.append(f\"<extra_id_{mask_counter}>\")\n",
    "\n",
    "        # The tokenizer handles spaces correctly, so we can join with a space\n",
    "        input_str = \" \".join(input_parts)\n",
    "        target_str = \" \".join(target_parts)\n",
    "        \n",
    "        dataset.append(f\"{input_str}\\t{target_str}\")\n",
    "        pbar.update(1)\n",
    "        \n",
    "    pbar.close()\n",
    "    with open(PRETRAIN_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(dataset))\n",
    "\n",
    "def create_finetune_dataset(functions):\n",
    "    \"\"\"Creates the if-statement masking dataset.\"\"\"\n",
    "    print(f\"Creating fine-tuning dataset with {FINETUNE_TARGET_SIZE} instances...\")\n",
    "    dataset = []\n",
    "    pbar = tqdm(total=FINETUNE_TARGET_SIZE)\n",
    "    \n",
    "    # Simple regex to find `if ...:` statements\n",
    "    if_regex = re.compile(r'if\\s+(.+?):')\n",
    "    \n",
    "    random.shuffle(functions)\n",
    "    for func in functions:\n",
    "        matches = list(if_regex.finditer(func))\n",
    "        if not matches:\n",
    "            continue\n",
    "            \n",
    "        match = random.choice(matches)\n",
    "        condition = match.group(1).strip()\n",
    "        \n",
    "        # Replace only the first occurrence of this specific match\n",
    "        masked_func = func[:match.start(1)] + MASK_TOKEN + func[match.end(1):]\n",
    "        \n",
    "        dataset.append(f\"{masked_func}\\t{condition}\")\n",
    "        pbar.update(1)\n",
    "        if len(dataset) >= FINETUNE_TARGET_SIZE:\n",
    "            break\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Split and save\n",
    "    train_end = int(len(dataset) * 0.8)\n",
    "    valid_end = int(len(dataset) * 0.9)\n",
    "    \n",
    "    with open(FINETUNE_TRAIN_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(dataset[:train_end]))\n",
    "    with open(FINETUNE_VALID_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(dataset[train_end:valid_end]))\n",
    "    with open(FINETUNE_TEST_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(dataset[valid_end:]))\n",
    "\n",
    "# --- 5. MODEL TRAINING ---\n",
    "\n",
    "def run_training(is_pretraining):\n",
    "    \"\"\"A general function to run either pre-training or fine-tuning.\"\"\"\n",
    "    \n",
    "    model_dir = PRETRAINED_MODEL_DIR if is_pretraining else FINETUNED_MODEL_DIR\n",
    "    train_file = PRETRAIN_FILE if is_pretraining else FINETUNE_TRAIN_FILE\n",
    "    valid_file = PRETRAIN_FILE if is_pretraining else FINETUNE_VALID_FILE # Use train as dummy valid for pretrain\n",
    "    \n",
    "    print(f\"\\n--- Starting {'Pre-training' if is_pretraining else 'Fine-tuning'} ---\")\n",
    "\n",
    "    # Load custom tokenizer\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=str(TOKENIZER_PATH))\n",
    "    # FIX: Explicitly set the padding token to fix the ValueError\n",
    "    tokenizer.pad_token = \"<pad>\"\n",
    "    \n",
    "    # Load or initialize model\n",
    "    if is_pretraining:\n",
    "        config = T5Config(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            d_model=256,\n",
    "            d_ff=1024,\n",
    "            num_layers=4,\n",
    "            num_heads=4,\n",
    "            decoder_start_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "        model = T5ForConditionalGeneration(config)\n",
    "    else:\n",
    "        # Load the model we just pre-trained\n",
    "        model = T5ForConditionalGeneration.from_pretrained(PRETRAINED_MODEL_DIR)\n",
    "\n",
    "    # Load and process dataset\n",
    "    dataset = load_dataset('text', data_files={'train': str(train_file), 'validation': str(valid_file)})\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        source, target = [], []\n",
    "        for line in examples['text']:\n",
    "            if '\\t' in line:\n",
    "                s, t = line.split('\\t', 1)\n",
    "                source.append(s)\n",
    "                target.append(t)\n",
    "        \n",
    "        # The prefix is helpful for T5\n",
    "        prefix = \"complete the python code: \"\n",
    "        source = [prefix + s for s in source]\n",
    "\n",
    "        # FIX: Reduced max_length to lower memory usage\n",
    "        model_inputs = tokenizer(source, max_length=256, padding=\"max_length\", truncation=True)\n",
    "        labels = tokenizer(target, max_length=128, padding=\"max_length\", truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=str(model_dir),\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=16,\n",
    "        num_train_epochs=3 if is_pretraining else 5,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=2000,\n",
    "        save_steps=2000,\n",
    "        learning_rate=5e-4 if is_pretraining else 1e-4,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=2,\n",
    "        predict_with_generate=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=500,\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.train()\n",
    "    trainer.save_model()\n",
    "    print(f\"--- {'Pre-training' if is_pretraining else 'Fine-tuning'} Complete ---\")\n",
    "\n",
    "\n",
    "# --- 6. EVALUATION ---\n",
    "\n",
    "def load_provided_test_set(file_path):\n",
    "    \"\"\"Loads and parses the specifically formatted provided test CSV.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['code'].tolist()\n",
    "\n",
    "def evaluate_and_create_csv(model, tokenizer, test_data, output_filename):\n",
    "    \"\"\"Runs model predictions and saves results to a CSV file.\"\"\"\n",
    "    print(f\"Evaluating and creating {output_filename}...\")\n",
    "    results = []\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    if_regex = re.compile(r'if\\s+(.+?):')\n",
    "\n",
    "    for code in tqdm(test_data, desc=f\"Predicting for {output_filename}\"):\n",
    "        matches = list(if_regex.finditer(code))\n",
    "        if not matches:\n",
    "            continue\n",
    "            \n",
    "        # For simplicity, always use the first if statement for evaluation\n",
    "        match = matches[0]\n",
    "        expected_condition = match.group(1).strip()\n",
    "        \n",
    "        input_code = code[:match.start(1)] + MASK_TOKEN + code[match.end(1):]\n",
    "        \n",
    "        # Generate prediction\n",
    "        prefix = \"complete the python code: \"\n",
    "        inputs = tokenizer(prefix + input_code, return_tensors=\"pt\", max_length=256, truncation=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_length=128,\n",
    "                num_beams=5,\n",
    "                early_stopping=True,\n",
    "                output_scores=True,\n",
    "                return_dict_in_generate=True\n",
    "            )\n",
    "        \n",
    "        predicted_condition = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Calculate score (confidence)\n",
    "        # We take the average log probability of the generated sequence\n",
    "        seq_log_prob = output.sequences_scores[0].item()\n",
    "        score = (torch.exp(torch.tensor(seq_log_prob)).item()) * 100 # Convert log prob to prob and scale to 0-100\n",
    "\n",
    "        results.append({\n",
    "            \"Input provided to the model\": input_code,\n",
    "            \"Whether the prediction is correct\": predicted_condition == expected_condition,\n",
    "            \"Expected if condition\": expected_condition,\n",
    "            \"Predicted if condition\": predicted_condition,\n",
    "            \"Prediction score (0-100)\": score\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(results).to_csv(output_filename, index=False)\n",
    "    print(f\"Successfully saved results to {output_filename}\")\n",
    "\n",
    "\n",
    "# --- 7. MAIN EXECUTION ---\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # --- Step 1: Collect and Process Data ---\n",
    "    if not list(RAW_CODE_DIR.glob(\"*.py\")):\n",
    "        repo_names = get_popular_python_repos()\n",
    "        for repo_name in tqdm(repo_names, desc=\"Downloading Repos\"):\n",
    "            py_files = get_python_files_from_repo(repo_name)\n",
    "            for file_path in tqdm(py_files, desc=f\"Files in {repo_name}\", leave=False):\n",
    "                download_and_save_code(repo_name, file_path)\n",
    "    \n",
    "    if not ALL_FUNCTIONS_FILE.exists():\n",
    "        all_funcs = []\n",
    "        for code_file in tqdm(list(RAW_CODE_DIR.glob(\"*.py\")), desc=\"Extracting Functions\"):\n",
    "            with open(code_file, 'r', encoding='utf-8') as f:\n",
    "                all_funcs.extend(extract_functions_from_code(f.read()))\n",
    "        \n",
    "        with open(ALL_FUNCTIONS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(list(set(all_funcs)))) # Use set for deduplication\n",
    "        print(f\"Extracted and saved {len(all_funcs)} functions.\")\n",
    "    else:\n",
    "        with open(ALL_FUNCTIONS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            all_funcs = f.read().splitlines()\n",
    "\n",
    "    # --- Step 2: Train Tokenizer ---\n",
    "    if not TOKENIZER_PATH.exists():\n",
    "        train_tokenizer(ALL_FUNCTIONS_FILE)\n",
    "\n",
    "    # --- Step 3: Create Datasets ---\n",
    "    if not PRETRAIN_FILE.exists() or not FINETUNE_TRAIN_FILE.exists():\n",
    "        # Correctly load the new tokenizer from the .json file\n",
    "        tokenizer = Tokenizer.from_file(str(TOKENIZER_PATH))\n",
    "        create_pretrain_dataset(all_funcs, tokenizer)\n",
    "        create_finetune_dataset(all_funcs)\n",
    "    \n",
    "    # --- Step 4: Pre-train the Model ---\n",
    "    if not (PRETRAINED_MODEL_DIR / \"pytorch_model.bin\").exists():\n",
    "         run_training(is_pretraining=True)\n",
    "\n",
    "    # --- Step 5: Fine-tune the Model ---\n",
    "    if not (FINETUNED_MODEL_DIR / \"pytorch_model.bin\").exists():\n",
    "        run_training(is_pretraining=False)\n",
    "    \n",
    "    # --- Step 6: Evaluate and Generate Submission Files ---\n",
    "    print(\"\\n--- Starting Final Evaluation ---\")\n",
    "    final_model = T5ForConditionalGeneration.from_pretrained(FINETUNED_MODEL_DIR)\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=str(TOKENIZER_PATH))\n",
    "    \n",
    "    # Evaluate on the test set we generated\n",
    "    with open(FINETUNE_TEST_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        generated_test_data = [line.split('\\t')[0] for line in f.read().splitlines()]\n",
    "    evaluate_and_create_csv(final_model, tokenizer, generated_test_data, \"generated-testset.csv\")\n",
    "    \n",
    "    # Evaluate on the test set provided by the user\n",
    "    if PROVIDED_TESTSET_PATH.exists():\n",
    "        provided_test_data = load_provided_test_set(PROVIDED_TESTSET_PATH)\n",
    "        evaluate_and_create_csv(final_model, tokenizer, provided_test_data, \"provided-testset.csv\")\n",
    "    else:\n",
    "        print(f\"\\nWarning: Could not find provided test set at '{PROVIDED_TESTSET_PATH}'. Skipping its evaluation.\")\n",
    "\n",
    "    print(\"\\n--- PIPELINE COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0e4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
